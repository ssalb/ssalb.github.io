<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Test-Time Compute: My Take on Story Generation | Dr. Salvador Salazar </title> <meta name="author" content="Dr. Salvador Salazar"> <meta name="description" content="Exploring test-time compute and beam search with a story generator app"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9B%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ssalb.github.io/blog/2025/test-time-compute-story-generation/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dr. Salvador</span> Salazar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Test-Time Compute: My Take on Story Generation</h1> <p class="post-meta"> Created in January 07, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a> ¬† <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> llms</a> ¬† <a href="/blog/tag/test-time-compute"> <i class="fa-solid fa-hashtag fa-sm"></i> test-time-compute</a> ¬† <a href="/blog/tag/beam-search"> <i class="fa-solid fa-hashtag fa-sm"></i> beam-search</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-01-07-story-gen/tree_diagram.webp" sizes="95vw"></source> <img src="/assets/img/blog/2025-01-07-story-gen/tree_diagram.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We‚Äôve seen incredible progress in large language models (LLMs) over the past few years, driven largely by scaling up model sizes and training data. But as noted in a <a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute" rel="external nofollow noopener" target="_blank">recent blog post by Hugging Face</a>, we‚Äôre starting to hit some hard limits‚Äîtraining these massive models requires billion-dollar compute clusters, and we‚Äôre also running out of high-quality training data.</p> <p>Enter test-time compute: instead of building ever-larger models, what if we let smaller models ‚Äúthink longer‚Äù about hard problems? Recent research shows this approach can be remarkably effective. In a <a href="https://arxiv.org/pdf/2408.03314" rel="external nofollow noopener" target="_blank">paper by Google DeepMind</a> published last August, the team demonstrates that with smart test-time compute allocation, smaller models can actually outperform models 14 times their size on certain tasks. Probably the most famous examples are OpenAI‚Äôs o1 and o3 models, but they‚Äôre not the only ones that can do these tricks: Deep Mind‚Äôs results were also replicated by the Hugging Face team as described in the blog post mentioned above.</p> <p>I wanted to get more familiar with these concepts, so I decided to explore them with a simple and fun toy project: story generation. Could test-time computing help a modest LLM write better stories by carefully evaluating and refining its outputs?</p> <p>Well‚Ä¶ yes! at least compared to the base model output. I have a demo under <a href="https://huggingface.co/spaces/ssalb/story_generator" rel="external nofollow noopener" target="_blank">this space</a>. It‚Äôll be live for a few days, but feel free to clone it later and run it yourself. The source code is also available in <a href="https://github.com/ssalb/story-beam-search" rel="external nofollow noopener" target="_blank">this GitHub repo</a>.</p> <hr> <h2 id="the-building-blocks-beam-search-and-quality-metrics">The Building Blocks: Beam Search and Quality Metrics</h2> <p>My approach uses beam search, a method that explores multiple possible story variations simultaneously instead of generating just one. Think of it like a chess player considering different moves before committing to one. Concretely, it does this by expanding each partial story with a number of possible next paths (beams) and scoring them; it then keeps only the top-scoring beams (the beam width) and moves on to repeat this process for a determined number of steps. This way, the model stays focused on the most promising story paths while still branching out enough to discover interesting possibilities. But I needed a way for the system to judge which stories were ‚Äúbetter,‚Äù so I implemented three quality metrics:</p> <ul> <li> <strong>Coherence</strong>: Do sentences flow naturally from one to the next? I evaluate this using cosine similarity between adjacent sentences.</li> <li> <strong>Fluency</strong>: Does the text read smoothly and naturally? To approximate this, I used BERT to evaluate the probability of each generated token based on the previous tokens (by masking them). (Disclaimer: I really wanted to give <a href="https://huggingface.co/answerdotai/ModernBERT-base" rel="external nofollow noopener" target="_blank">ModernBERT</a> a try, so this biased my approach significantly. üòâ)</li> <li> <strong>Genre alignment</strong>: Does the story match its intended genre? I used a zero-shot classifier to predict whether a piece of text could be labeled by a predefined genre.</li> </ul> <p>Writing a functional app took about 10% of the time at most, and the CI pipeline to get it running on a Hugging Face space another 10%. In contrast, about 40% of the time I invested was spent experimenting with these scoring algorithms. I tried different normalizations and ways of splitting the stories. For fluency, for example, I use the whole story as one unit. For genre alignment, I found it worked better by splitting stories into sentences and classifying them independently‚Äîthis helped control alignment throughout the story.</p> <p>Fun fact: the genre alignment scorer turned out to be a decently good, low-effort prompt injection mechanism. I check the probability of the initial prompt being classified as ‚Äústory‚Äù versus ‚Äúprompt injection.‚Äù If the ‚Äústoryness‚Äù is too low or the ‚Äúprompt injection‚Äù probability too high, I stop the process before generating anything. It‚Äôs by no means perfect, and I‚Äôve definitely seen a few false positives, but it‚Äôs surprisingly effective for a quick demo.</p> <h2 id="making-beam-search-work-for-creative-tasks">Making Beam Search Work for Creative Tasks</h2> <p>One interesting challenge emerged: basic beam search seems great at finding the ‚Äúoptimal‚Äù solution but not so great at generating diverse, creative outputs‚Äîmy first attempts returned stories that were way too similar, sometimes differing by only a single word. Taking inspiration from the <a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute" rel="external nofollow noopener" target="_blank">blog post</a> mentioned earlier (see the DVTS section), I modified the algorithm to maintain independent ‚Äúbeams‚Äù of stories that develop separately, rather than constantly comparing and filtering them all together. This helped preserve more variety in the outputs.</p> <h2 id="the-computing-challenge">The Computing Challenge</h2> <p>Were you wondering where I spent the other 40% of my time? Well, here‚Äôs where it gets interesting: while generating a single story from a language model takes just a couple of seconds on a GPU (I tested on a T4), the full test-time computation process is much more intensive. With a modest-sized (and not particularly impressive anymore) model like GPT2, the entire process takes 3‚Äì5 minutes. Scale up to Llama 3.2 1B (my original target model) and you‚Äôll get significantly better stories, but you‚Äôll also be looking at 30+ minutes of processing time! That‚Äôs why I scaled it down for my demo.</p> <p>I‚Äôm sure my code can be optimized further, but even after some thorough refactoring to process data in batches wherever possible, I only improved runtime by about 20%. The iterative steps just take a while‚Äîat least compared to what we‚Äôre used to when using LLMs directly. I believe researchers are putting effort into embedding these search algorithms in the network architectures themselves, but I‚Äôm not an expert in this field, so don‚Äôt quote me on that!</p> <hr> <h2 id="what-i-learned">What I Learned</h2> <p>Test-time computing offers a fascinating alternative to the ‚Äúbigger is better‚Äù mindset that often dominates LLM and AI development. While it does introduce significant computational overhead, it enables smaller models to produce higher-quality outputs through careful evaluation and refinement. Imagine how ‚Äúsmall model, big compute‚Äù could also help with things like dialogue systems, or specialized domain tasks; anything really that would benefit from ‚Äúthinking longer and in steps‚Äù.</p> <p>Of course, we have to acknowledge the elephant in the room: more inference steps cost more time, so if you need super-quick responses, you might be better off with bigger models. In practice, it will probably be a combination of both‚Äìas we keep pushing the limits of model scaling, these methods for making smarter use of inference-time compute will likely become increasingly important‚Äîespecially if new algorithms and hardware optimizations can streamline the process.</p> <p>For story generation specifically, in this experiment beam search seems to be very effective at finding ‚Äúoptimal‚Äù stories according to our metrics‚Äîthough maintaining creativity and diversity requires some clever tweaks to the standard approach. Perhaps other methods that would preference exploration over exploitation would work better. I do have a soft spot for Monte Carlo methods, so I‚Äôll try to find another <del>excuse</del> project to use a MC tree search next.</p> <h3 id="takeaways">Takeaways</h3> <ul> <li> <strong>Small Models, Big Thinking</strong>: With enough ‚Äúthinking time,‚Äù a modest LLM can punch above its weight class.</li> <li> <strong>Balancing Creativity &amp; Optimality</strong>: Beam search seems to be better at finding an optimal solution. Other methods might preference exploration more.</li> <li> <strong>Real-World Viability</strong>: Extra processing time isn‚Äôt always ideal, but for certain applications it can be worth the trade-off.</li> </ul> <p>Feel free to tinker with the code, clone the project, or adapt it for your own experiments. I‚Äôd love to hear about your successes, hiccups, and any wild new ideas you come up with!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/cgan-class-balancer/">A Conditional GAN for Data Augmentation: A Cautionary Tale</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/company-no-ml/">If your company is not doing ML, it (probably) won‚Äôt start any time soon either</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Dr. Salvador Salazar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PTQHG0B0SV"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PTQHG0B0SV');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>