<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Conditional GAN for Data Augmentation: A Cautionary Tale | Dr. Salvador Salazar </title> <meta name="author" content="Dr. Salvador Salazar"> <meta name="description" content="Balancing an image classification dataset using synthetic images"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9B%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ssalb.github.io/blog/2025/cgan-class-balancer/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dr. Salvador</span> Salazar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Conditional GAN for Data Augmentation: A Cautionary Tale</h1> <p class="post-meta"> Created in February 23, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/gan"> <i class="fa-solid fa-hashtag fa-sm"></i> GAN</a>   <a href="/blog/tag/image-classification"> <i class="fa-solid fa-hashtag fa-sm"></i> image-classification</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/keras"> <i class="fa-solid fa-hashtag fa-sm"></i> keras</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In recent years, the use of synthetic data has become increasingly important for improving machine learning models, especially in situations where data is scarce, sensitive, or simply when it becomes impractical to keep scaling real-world data. Major tech companies like <a href="https://www.businessinsider.com/ai-synthetic-data-industry-debate-over-fake-2024-8" rel="external nofollow noopener" target="_blank">Google, OpenAI</a>, and particularly Nvidia with the release of <a href="https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/" rel="external nofollow noopener" target="_blank">Nemotron</a> and their <a href="https://nvidianews.nvidia.com/news/nvidia-expands-omniverse-with-generative-physical-ai" rel="external nofollow noopener" target="_blank">Omniverse platform</a>, among others, have been investing heavily in this. It is also an active research area in academia, with <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" rel="external nofollow noopener" target="_blank">Stanford’s Alpaca</a> probably being the first recent example that comes to mind for most people in the field.</p> <p>This is the backdrop for this project, where I set out to answer the following question:</p> <p><strong>For an image classification task, can generated images improve model performance on an imbalanced dataset?</strong></p> <p>In other words, does rebalancing an imbalanced dataset by sampling synthetic images for minority classes improve classification accuracy—all while staying on a private, consumer-grade budget (because I’m not a research lab 😜)?</p> <p>To this end, I used an unbalanced version of the EuroSAT dataset, trained a conditional GAN to generate new synthetic samples conditioned on a given class, and fine-tuned a pre-trained <em>EfficientNetB2</em> on different versions of this dataset. I trained all models on a rather modest instance from AWS SageMaker, while some other tasks were run on my personal PC. If you’re interested, the code and scripts are all available under <a href="https://github.com/ssalb/cgan-class-balancer" rel="external nofollow noopener" target="_blank">this GitHub repo</a>.</p> <p><em>Spoiler alert: not everything went as planned, but those unexpected twists made the journey all the more interesting!</em></p> <hr> <h2 id="dataset-overview">Dataset Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/example_images-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/example_images-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/example_images-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025-02-23-cgan-class/example_images.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Examples from the EuroSAT dataset. Not every class is displayed. </div> <p><a href="https://github.com/phelber/EuroSAT" rel="external nofollow noopener" target="_blank">EuroSAT</a> consists of 10 classes of land-use/land-cover from ESA’s Sentinel-2 satellite images <a href="https://arxiv.org/abs/1709.00029" rel="external nofollow noopener" target="_blank">(Helber et al., 2019)</a>, with 27,000+ total images (RGB or multispectral). Classes include:</p> <ul> <li>Annual Crop</li> <li>Forest</li> <li>Herbaceous Vegetation</li> <li>Highway</li> <li>Industrial Buildings</li> <li>Pasture</li> <li>Permanent Crop</li> <li>Residential Buildings</li> <li>River</li> <li>SeaLake</li> </ul> <p>In particular I’ve used <a href="https://huggingface.co/datasets/blanchon/EuroSAT_RGB" rel="external nofollow noopener" target="_blank">this version</a> from Hugging Face’s Datasets, which only contains the RGB images and already comes with a train-validation-test split.</p> <p>The original dataset is well balanced, so I artificially reduced two classes–<em>Highway</em> and <em>River</em>–by 85% in the training set, leaving the validation and test sets untouched. I picked these two becuase they show quite distinct features, unlilke, say, <em>Forest</em> or <em>SeaLake</em>, wich can often be very flat, but they’re also not as complex as <em>Industrial Buildings</em> or <em>Residential Buildings</em> can be.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/eurosat_training_distr-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/eurosat_training_distr-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/eurosat_training_distr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025-02-23-cgan-class/eurosat_training_distr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/eurosat_unbalanced_distr-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/eurosat_unbalanced_distr-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/eurosat_unbalanced_distr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025-02-23-cgan-class/eurosat_unbalanced_distr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The class counts of the original training set (left) and the atrificially unbalanced one (right) with the classes <i>Highway</i> and <i>River</i> undersampled by 85% </div> <h2 id="experiment-design">Experiment Design</h2> <p>Throughout this experiment, I’ve used the <a href="https://keras.io/about/" rel="external nofollow noopener" target="_blank">Keras 3</a> framework with TensorFlow as the backend. I’ve tried to keep my code as “backend agnostic” as possible, and most models should actually work with other supported backends (currently PyTorch and JAX). The only exception should be the custom training loop for the GAN. If you’re interested in optimizers and hyperparameters, check out the code.</p> <p>Here’s the plan:</p> <h4 id="baseline">Baseline</h4> <p>Fine-tune a pretrained <em>EfficientNetB2</em> classifier using both the original and the unbalanced dataset, and use the classification accuracy on the test set as the metric to compare different models. Train multiple models using different random seeds and calculate an average score for each.</p> <h4 id="data-augmentation">Data Augmentation</h4> <p>Build a conditional GAN model and train the generator to produce images of all 10 classes, conditioned on the class label as input. Use the generator to create images of the two minority classes. Using the <em>unbalanced</em> classifier mentioned above, pass these generated images through it, keeping only those that the model correctly classifies as belonging to the same class as the intended one (GAN input). Generate 500 new images for each class and build three new training sets from these:</p> <ul> <li><strong>augmented100</strong></li> <li><strong>augmented300</strong></li> <li><strong>augmented500</strong></li> </ul> <p>Each augments the unbalanced training set by 100, 300, and 500 images, respectively.</p> <h4 id="evaluation">Evaluation</h4> <p>Fine-tune multiple classifiers for each augmented dataset, calculate the group accuracy score, and compare it to the baseline. Additionally, calculate the per-class group accuracy for the minority classes across all trained classifiers and compare them.</p> <hr> <h2 id="the-gan-rabbit-hole-️">The GAN Rabbit Hole 🕳️</h2> <p>So far, so good. I had my dataset, experiment design, and infrastructure (first time using Pulumi) set up. My unbalanced dataset was ready, and I had fine-tuned a classifier to establish a baseline. Time to generate some synthetic data!</p> <p>I started with <a href="https://keras.io/examples/generative/conditional_gan" rel="external nofollow noopener" target="_blank">this Keras example</a>, modifying it for image dimensions and channels (RGB). My first training run on the <em>unbalanced</em> dataset wasn’t good but produced exactly what I expected—blurry patches of green, brown, and blue, vaguely resembling satellite imagery.</p> <p><em>Great!</em> – I thought – <em>this is going in the right direction.</em> I tweaked hyperparameters systematically, adding a callback to generate samples every 10 epochs. After many trials, I still had little more than color-matched patches, maybe some crop-like structures—if I squinted. So, I explored deeper networks, different upsampling methods, and better class conditioning.</p> <p>That led me into researching more advanced GANs. I added an embedding layer for class labels, learned about projection discriminators <a href="https://arxiv.org/abs/1802.05637" rel="external nofollow noopener" target="_blank">(Miyato &amp; Koyama, 2018)</a>, and experimented with Wasserstein GANs with gradient penalty <a href="https://arxiv.org/abs/1704.00028" rel="external nofollow noopener" target="_blank">(WGAN-GP; Gulrajani et al., 2017)</a>. Each change meant longer training runs, but at least I never had to upgrade my instance 😅.</p> <p>Despite all the effort, nothing was working, my AWS bill was climbing, and I was running out of ideas. Then it hit me—EuroSAT is a well-known dataset. Surely, someone had tried this before? Sure enough, I found <a href="https://www.cs.swarthmore.edu/~llwin1/files/labelled_sat_images_dcgan_lwin.pdf" rel="external nofollow noopener" target="_blank">a paper</a> from (probably) 2022 Swarthmore students who had used a similar model. I replicated their setup and… nothing!</p> <p>But this was the turning point. I had proof that it <em>was</em> possible, just not with my dataset. So, I finally did what I should’ve done earlier: trained a GAN on the <em>full</em> dataset. First try… <strong>it worked!</strong></p> <h4 id="the-end-of-my-gan-learning-journey">The End of My GAN Learning Journey</h4> <p>The issue was the dataset imbalance all along. I had tried class weights but never focused on balancing techniques like oversampling.</p> <p>At this point, I decided to use the GAN trained on the full dataset. Sure, it weakened the project’s real-world applicability—the whole goal was to handle imbalance with synthetic data—but solving that problem was an entirely new project. The right call was to finish answering my original question and leave that challenge for another day.</p> <p>On the bright side, I learned so much about GANs, conditioning them and stabilizing training, so no regrets!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/generated_highway-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/generated_highway-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/generated_highway-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025-02-23-cgan-class/generated_highway.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Generated samples of the <i>Highway</i> class. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/generated_river-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/generated_river-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/generated_river-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025-02-23-cgan-class/generated_river.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Generated samples of the <i>River</i> class. </div> <hr> <h2 id="final-results">Final Results</h2> <p>At this point, I had spent way more time (and money on AWS) than I had planned. For this reason I decided to adapt my analysis: I would train only 3 classifiers per setup. The issue was that this wouldn’t be near enough samples for calculating proper group statistics, so I ended up pulling out a trick.</p> <h4 id="estimating-variance-with-pooling">Estimating Variance with Pooling</h4> <p>Let \(x^i_j\) be the accuracy score of the \(j\)-th trained classifier (\(j \in [1,3]\)) for dataset \(i \in G\), where \(G\) is the set of datasets (or groups) mentioned above. I’ll also add to \(G\) a variant that fine-tunes the classifier on the <em>unbalanced</em> dataset with class weights to compensate for imbalance.</p> <p>Let’s make two assumptions:</p> <ul> <li>Three samples are enough to estimate the mean of each group (outrageous, I know).</li> <li>All groups share the same (symetric) accuracy score distribution, just shifted by their mean.</li> </ul> <p>With this, we can subtract the group’s mean accuracy \(\mu^i\) from each sample, obtaining \(\tilde{x}^i_j = x^i_j - \mu^i\). By doing so, we’ve effectively centered all distributions around zero. Since we assumed they were the same distribution but shifted, we can now “borrow power” from the other samples and calculate a shared variance as:</p> \[\sigma^2 = \frac{1}{2|G|}\sum_{i\in G}\sum_{j=1}^{3}(\tilde{x}^i_j)^2\] <p>This is of course a simplification, it is meant to help visualize differences in the mean results and not to rigorously account for all sources of uncertainty.</p> <h4 id="performance-on-the-test-set">Performance on the Test Set</h4> <p>Finally! Do these generated images improve the classifier’s accuracy on the test set? Well, they do seem to help, but not more than simply training on the unbalanced dataset while giving more weight to the minority classes. But that result alone is a bit misleading. What caught me off guard was that when looking at the mean accuracy on the minority classes only, adding synthetic images actually performs worse.</p> <p>This result might seem strange at first—it certainly did to me—but after thinking it through, I have a hypothesis. I suspect that, on the one hand, the generated samples lack variability and fail to capture all the details of the real data. On the other hand, they still upsample the minority classes. The effect is that the more synthetic images in the dataset, the more the model overfits on them, reducing per-class accuracy for these classes while smoothing out the decision boundaries for the majority classes, improving their accuracy. The net effect is positive, which is why the overall accuracy improves.</p> <p>If this hypothesis is correct—and considering that I trained the GAN on the full dataset—it suggests that effectively modeling the synthetic data is critical for dataset augmentation to work this way. Proving this hypothesis, though, is a whole project in itself, so I won’t attempt it today.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/test_set_performance-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/test_set_performance-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/test_set_performance-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025-02-23-cgan-class/test_set_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/per_class_performance-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/per_class_performance-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/per_class_performance-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025-02-23-cgan-class/per_class_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Mean classification accuracy on the full test set (left) and only considering the minority classes <i>Highway</i> and <i>River</i> (right) for after fine-tuning on each dataset labeled in the x-axis. </div> <h4 id="conclusion">Conclusion</h4> <p>This project started as a straightforward experiment in data augmentation but quickly turned into a deep dive into GANs, dataset balancing, and the unexpected ways synthetic data interacts with model training. While the generated images did improve overall accuracy, they didn’t outperform a simpler approach like class weighting—and, in fact, they hurt performance on the minority classes. That was an unexpected twist, but one that makes sense in hindsight: more data isn’t always better if it doesn’t capture the full complexity of the real thing.</p> <p>There are plenty of open questions left—like whether a better GAN model, or perhaps a difussion model, trained directly on the unbalanced dataset could have made a difference. That’s a problem for another day. For now, this was a fun (if occasionally frustrating) exploration, and I’ve learned a lot along the way. And at the very least, I now have a much better understanding of why dataset augmentation is trickier than it seems.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/test-time-compute-story-generation/">Test-Time Compute: My Take on Story Generation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/company-no-ml/">If your company is not doing ML, it (probably) won’t start any time soon either</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dr. Salvador Salazar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PTQHG0B0SV"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PTQHG0B0SV');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>