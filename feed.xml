<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ssalb.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ssalb.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-04T11:16:40+00:00</updated><id>https://ssalb.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">A Conditional GAN for Data Augmentation: A Cautionary Tale</title><link href="https://ssalb.github.io/blog/2025/cgan-class-balancer/" rel="alternate" type="text/html" title="A Conditional GAN for Data Augmentation: A Cautionary Tale"/><published>2025-02-23T09:00:00+00:00</published><updated>2025-02-23T09:00:00+00:00</updated><id>https://ssalb.github.io/blog/2025/cgan-class-balancer</id><content type="html" xml:base="https://ssalb.github.io/blog/2025/cgan-class-balancer/"><![CDATA[<p>In recent years, the use of synthetic data has become increasingly important for improving machine learning models, especially in situations where data is scarce, sensitive, or simply when it becomes impractical to keep scaling real-world data. Major tech companies like <a href="https://www.businessinsider.com/ai-synthetic-data-industry-debate-over-fake-2024-8">Google, OpenAI</a>, and particularly Nvidia with the release of <a href="https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/">Nemotron</a> and their <a href="https://nvidianews.nvidia.com/news/nvidia-expands-omniverse-with-generative-physical-ai">Omniverse platform</a>, among others, have been investing heavily in this. It is also an active research area in academia, with <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford‚Äôs Alpaca</a> probably being the first recent example that comes to mind for most people in the field.</p> <p>This is the backdrop for this project, where I set out to answer the following question:</p> <p><strong>For an image classification task, can generated images improve model performance on an imbalanced dataset?</strong></p> <p>In other words, does rebalancing an imbalanced dataset by sampling synthetic images for minority classes improve classification accuracy‚Äîall while staying on a private, consumer-grade budget (because I‚Äôm not a research lab üòú)?</p> <p>To this end, I used an unbalanced version of the EuroSAT dataset, trained a conditional GAN to generate new synthetic samples conditioned on a given class, and fine-tuned a pre-trained <em>EfficientNetB2</em> on different versions of this dataset. I trained all models on a rather modest instance from AWS SageMaker, while some other tasks were run on my personal PC. If you‚Äôre interested, the code and scripts are all available under <a href="https://github.com/ssalb/cgan-class-balancer">this GitHub repo</a>.</p> <p><em>Spoiler alert: not everything went as planned, but those unexpected twists made the journey all the more interesting!</em></p> <hr/> <h2 id="dataset-overview">Dataset Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/example_images-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/example_images-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/example_images-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025-02-23-cgan-class/example_images.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Examples from the EuroSAT dataset. Not every class is displayed. </div> <p><a href="https://github.com/phelber/EuroSAT">EuroSAT</a> consists of 10 classes of land-use/land-cover from ESA‚Äôs Sentinel-2 satellite images <a href="https://arxiv.org/abs/1709.00029">(Helber et al., 2019)</a>, with 27,000+ total images (RGB or multispectral). Classes include:</p> <ul> <li>Annual Crop</li> <li>Forest</li> <li>Herbaceous Vegetation</li> <li>Highway</li> <li>Industrial Buildings</li> <li>Pasture</li> <li>Permanent Crop</li> <li>Residential Buildings</li> <li>River</li> <li>SeaLake</li> </ul> <p>In particular I‚Äôve used <a href="https://huggingface.co/datasets/blanchon/EuroSAT_RGB">this version</a> from Hugging Face‚Äôs Datasets, which only contains the RGB images and already comes with a train-validation-test split.</p> <p>The original dataset is well balanced, so I artificially reduced two classes‚Äì<em>Highway</em> and <em>River</em>‚Äìby 85% in the training set, leaving the validation and test sets untouched. I picked these two becuase they show quite distinct features, unlilke, say, <em>Forest</em> or <em>SeaLake</em>, wich can often be very flat, but they‚Äôre also not as complex as <em>Industrial Buildings</em> or <em>Residential Buildings</em> can be.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/eurosat_training_distr-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/eurosat_training_distr-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/eurosat_training_distr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025-02-23-cgan-class/eurosat_training_distr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/eurosat_unbalanced_distr-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/eurosat_unbalanced_distr-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/eurosat_unbalanced_distr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025-02-23-cgan-class/eurosat_unbalanced_distr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The class counts of the original training set (left) and the atrificially unbalanced one (right) with the classes <i>Highway</i> and <i>River</i> undersampled by 85% </div> <h2 id="experiment-design">Experiment Design</h2> <p>Throughout this experiment, I‚Äôve used the <a href="https://keras.io/about/">Keras 3</a> framework with TensorFlow as the backend. I‚Äôve tried to keep my code as ‚Äúbackend agnostic‚Äù as possible, and most models should actually work with other supported backends (currently PyTorch and JAX). The only exception should be the custom training loop for the GAN. If you‚Äôre interested in optimizers and hyperparameters, check out the code.</p> <p>Here‚Äôs the plan:</p> <h4 id="baseline">Baseline</h4> <p>Fine-tune a pretrained <em>EfficientNetB2</em> classifier using both the original and the unbalanced dataset, and use the classification accuracy on the test set as the metric to compare different models. Train multiple models using different random seeds and calculate an average score for each.</p> <h4 id="data-augmentation">Data Augmentation</h4> <p>Build a conditional GAN model and train the generator to produce images of all 10 classes, conditioned on the class label as input. Use the generator to create images of the two minority classes. Using the <em>unbalanced</em> classifier mentioned above, pass these generated images through it, keeping only those that the model correctly classifies as belonging to the same class as the intended one (GAN input). Generate 500 new images for each class and build three new training sets from these:</p> <ul> <li><strong>augmented100</strong></li> <li><strong>augmented300</strong></li> <li><strong>augmented500</strong></li> </ul> <p>Each augments the unbalanced training set by 100, 300, and 500 images, respectively.</p> <h4 id="evaluation">Evaluation</h4> <p>Fine-tune multiple classifiers for each augmented dataset, calculate the group accuracy score, and compare it to the baseline. Additionally, calculate the per-class group accuracy for the minority classes across all trained classifiers and compare them.</p> <hr/> <h2 id="the-gan-rabbit-hole-Ô∏è">The GAN Rabbit Hole üï≥Ô∏è</h2> <p>So far, so good. I had my dataset, experiment design, and infrastructure (first time using Pulumi) set up. My unbalanced dataset was ready, and I had fine-tuned a classifier to establish a baseline. Time to generate some synthetic data!</p> <p>I started with <a href="https://keras.io/examples/generative/conditional_gan">this Keras example</a>, modifying it for image dimensions and channels (RGB). My first training run on the <em>unbalanced</em> dataset wasn‚Äôt good but produced exactly what I expected‚Äîblurry patches of green, brown, and blue, vaguely resembling satellite imagery.</p> <p><em>Great!</em> ‚Äì I thought ‚Äì <em>this is going in the right direction.</em> I tweaked hyperparameters systematically, adding a callback to generate samples every 10 epochs. After many trials, I still had little more than color-matched patches, maybe some crop-like structures‚Äîif I squinted. So, I explored deeper networks, different upsampling methods, and better class conditioning.</p> <p>That led me into researching more advanced GANs. I added an embedding layer for class labels, learned about projection discriminators <a href="https://arxiv.org/abs/1802.05637">(Miyato &amp; Koyama, 2018)</a>, and experimented with Wasserstein GANs with gradient penalty <a href="https://arxiv.org/abs/1704.00028">(WGAN-GP; Gulrajani et al., 2017)</a>. Each change meant longer training runs, but at least I never had to upgrade my instance üòÖ.</p> <p>Despite all the effort, nothing was working, my AWS bill was climbing, and I was running out of ideas. Then it hit me‚ÄîEuroSAT is a well-known dataset. Surely, someone had tried this before? Sure enough, I found <a href="https://www.cs.swarthmore.edu/~llwin1/files/labelled_sat_images_dcgan_lwin.pdf">a paper</a> from (probably) 2022 Swarthmore students who had used a similar model. I replicated their setup and‚Ä¶ nothing!</p> <p>But this was the turning point. I had proof that it <em>was</em> possible, just not with my dataset. So, I finally did what I should‚Äôve done earlier: trained a GAN on the <em>full</em> dataset. First try‚Ä¶ <strong>it worked!</strong></p> <h4 id="the-end-of-my-gan-learning-journey">The End of My GAN Learning Journey</h4> <p>The issue was the dataset imbalance all along. I had tried class weights but never focused on balancing techniques like oversampling.</p> <p>At this point, I decided to use the GAN trained on the full dataset. Sure, it weakened the project‚Äôs real-world applicability‚Äîthe whole goal was to handle imbalance with synthetic data‚Äîbut solving that problem was an entirely new project. The right call was to finish answering my original question and leave that challenge for another day.</p> <p>On the bright side, I learned so much about GANs, conditioning them and stabilizing training, so no regrets!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/generated_highway-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/generated_highway-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/generated_highway-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025-02-23-cgan-class/generated_highway.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Generated samples of the <i>Highway</i> class. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/generated_river-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/generated_river-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/generated_river-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025-02-23-cgan-class/generated_river.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Generated samples of the <i>River</i> class. </div> <hr/> <h2 id="final-results">Final Results</h2> <p>At this point, I had spent way more time (and money on AWS) than I had planned. For this reason I decided to adapt my analysis: I would train only 3 classifiers per setup. The issue was that this wouldn‚Äôt be near enough samples for calculating proper group statistics, so I ended up pulling out a trick.</p> <h4 id="estimating-variance-with-pooling">Estimating Variance with Pooling</h4> <p>Let \(x^i_j\) be the accuracy score of the \(j\)-th trained classifier (\(j \in [1,3]\)) for dataset \(i \in G\), where \(G\) is the set of datasets (or groups) mentioned above. I‚Äôll also add to \(G\) a variant that fine-tunes the classifier on the <em>unbalanced</em> dataset with class weights to compensate for imbalance.</p> <p>Let‚Äôs make two assumptions:</p> <ul> <li>Three samples are enough to estimate the mean of each group (outrageous, I know).</li> <li>All groups share the same (symetric) accuracy score distribution, just shifted by their mean.</li> </ul> <p>With this, we can subtract the group‚Äôs mean accuracy \(\mu^i\) from each sample, obtaining \(\tilde{x}^i_j = x^i_j - \mu^i\). By doing so, we‚Äôve effectively centered all distributions around zero. Since we assumed they were the same distribution but shifted, we can now ‚Äúborrow power‚Äù from the other samples and calculate a shared variance as:</p> \[\sigma^2 = \frac{1}{2|G|}\sum_{i\in G}\sum_{j=1}^{3}(\tilde{x}^i_j)^2\] <p>This is of course a simplification, it is meant to help visualize differences in the mean results and not to rigorously account for all sources of uncertainty.</p> <h4 id="performance-on-the-test-set">Performance on the Test Set</h4> <p>Finally! Do these generated images improve the classifier‚Äôs accuracy on the test set? Well, they do seem to help, but not more than simply training on the unbalanced dataset while giving more weight to the minority classes. But that result alone is a bit misleading. What caught me off guard was that when looking at the mean accuracy on the minority classes only, adding synthetic images actually performs worse.</p> <p>This result might seem strange at first‚Äîit certainly did to me‚Äîbut after thinking it through, I have a hypothesis. I suspect that, on the one hand, the generated samples lack variability and fail to capture all the details of the real data. On the other hand, they still upsample the minority classes. The effect is that the more synthetic images in the dataset, the more the model overfits on them, reducing per-class accuracy for these classes while smoothing out the decision boundaries for the majority classes, improving their accuracy. The net effect is positive, which is why the overall accuracy improves.</p> <p>If this hypothesis is correct‚Äîand considering that I trained the GAN on the full dataset‚Äîit suggests that effectively modeling the synthetic data is critical for dataset augmentation to work this way. Proving this hypothesis, though, is a whole project in itself, so I won‚Äôt attempt it today.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/test_set_performance-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/test_set_performance-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/test_set_performance-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025-02-23-cgan-class/test_set_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-02-23-cgan-class/per_class_performance-480.webp 480w,/assets/img/blog/2025-02-23-cgan-class/per_class_performance-800.webp 800w,/assets/img/blog/2025-02-23-cgan-class/per_class_performance-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025-02-23-cgan-class/per_class_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Mean classification accuracy on the full test set (left) and only considering the minority classes <i>Highway</i> and <i>River</i> (right) for after fine-tuning on each dataset labeled in the x-axis. </div> <h4 id="conclusion">Conclusion</h4> <p>This project started as a straightforward experiment in data augmentation but quickly turned into a deep dive into GANs, dataset balancing, and the unexpected ways synthetic data interacts with model training. While the generated images did improve overall accuracy, they didn‚Äôt outperform a simpler approach like class weighting‚Äîand, in fact, they hurt performance on the minority classes. That was an unexpected twist, but one that makes sense in hindsight: more data isn‚Äôt always better if it doesn‚Äôt capture the full complexity of the real thing.</p> <p>There are plenty of open questions left‚Äîlike whether a better GAN model, or perhaps a difussion model, trained directly on the unbalanced dataset could have made a difference. That‚Äôs a problem for another day. For now, this was a fun (if occasionally frustrating) exploration, and I‚Äôve learned a lot along the way. And at the very least, I now have a much better understanding of why dataset augmentation is trickier than it seems.</p>]]></content><author><name></name></author><category term="GAN"/><category term="image-classification"/><category term="deep-learning"/><category term="keras"/><summary type="html"><![CDATA[Balancing an image classification dataset using synthetic images]]></summary></entry><entry><title type="html">Test-Time Compute: My Take on Story Generation</title><link href="https://ssalb.github.io/blog/2025/test-time-compute-story-generation/" rel="alternate" type="text/html" title="Test-Time Compute: My Take on Story Generation"/><published>2025-01-07T09:00:00+00:00</published><updated>2025-01-07T09:00:00+00:00</updated><id>https://ssalb.github.io/blog/2025/test-time-compute-story-generation</id><content type="html" xml:base="https://ssalb.github.io/blog/2025/test-time-compute-story-generation/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025-01-07-story-gen/tree_diagram.webp" sizes="95vw"/> <img src="/assets/img/blog/2025-01-07-story-gen/tree_diagram.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We‚Äôve seen incredible progress in large language models (LLMs) over the past few years, driven largely by scaling up model sizes and training data. But as noted in a <a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute">recent blog post by Hugging Face</a>, we‚Äôre starting to hit some hard limits‚Äîtraining these massive models requires billion-dollar compute clusters, and we‚Äôre also running out of high-quality training data.</p> <p>Enter test-time compute: instead of building ever-larger models, what if we let smaller models ‚Äúthink longer‚Äù about hard problems? Recent research shows this approach can be remarkably effective. In a <a href="https://arxiv.org/pdf/2408.03314">paper by Google DeepMind</a> published last August, the team demonstrates that with smart test-time compute allocation, smaller models can actually outperform models 14 times their size on certain tasks. Probably the most famous examples are OpenAI‚Äôs o1 and o3 models, but they‚Äôre not the only ones that can do these tricks: Deep Mind‚Äôs results were also replicated by the Hugging Face team as described in the blog post mentioned above.</p> <p>I wanted to get more familiar with these concepts, so I decided to explore them with a simple and fun toy project: story generation. Could test-time computing help a modest LLM write better stories by carefully evaluating and refining its outputs?</p> <p>Well‚Ä¶ yes! at least compared to the base model output. I have a demo under <a href="https://huggingface.co/spaces/ssalb/story_generator">this space</a>. It‚Äôll be live for a few days, but feel free to clone it later and run it yourself. The source code is also available in <a href="https://github.com/ssalb/story-beam-search">this GitHub repo</a>.</p> <hr/> <h2 id="the-building-blocks-beam-search-and-quality-metrics">The Building Blocks: Beam Search and Quality Metrics</h2> <p>My approach uses beam search, a method that explores multiple possible story variations simultaneously instead of generating just one. Think of it like a chess player considering different moves before committing to one. Concretely, it does this by expanding each partial story with a number of possible next paths (beams) and scoring them; it then keeps only the top-scoring beams (the beam width) and moves on to repeat this process for a determined number of steps. This way, the model stays focused on the most promising story paths while still branching out enough to discover interesting possibilities. But I needed a way for the system to judge which stories were ‚Äúbetter,‚Äù so I implemented three quality metrics:</p> <ul> <li><strong>Coherence</strong>: Do sentences flow naturally from one to the next? I evaluate this using cosine similarity between adjacent sentences.</li> <li><strong>Fluency</strong>: Does the text read smoothly and naturally? To approximate this, I used BERT to evaluate the probability of each generated token based on the previous tokens (by masking them). (Disclaimer: I really wanted to give <a href="https://huggingface.co/answerdotai/ModernBERT-base">ModernBERT</a> a try, so this biased my approach significantly. üòâ)</li> <li><strong>Genre alignment</strong>: Does the story match its intended genre? I used a zero-shot classifier to predict whether a piece of text could be labeled by a predefined genre.</li> </ul> <p>Writing a functional app took about 10% of the time at most, and the CI pipeline to get it running on a Hugging Face space another 10%. In contrast, about 40% of the time I invested was spent experimenting with these scoring algorithms. I tried different normalizations and ways of splitting the stories. For fluency, for example, I use the whole story as one unit. For genre alignment, I found it worked better by splitting stories into sentences and classifying them independently‚Äîthis helped control alignment throughout the story.</p> <p>Fun fact: the genre alignment scorer turned out to be a decently good, low-effort prompt injection mechanism. I check the probability of the initial prompt being classified as ‚Äústory‚Äù versus ‚Äúprompt injection.‚Äù If the ‚Äústoryness‚Äù is too low or the ‚Äúprompt injection‚Äù probability too high, I stop the process before generating anything. It‚Äôs by no means perfect, and I‚Äôve definitely seen a few false positives, but it‚Äôs surprisingly effective for a quick demo.</p> <h2 id="making-beam-search-work-for-creative-tasks">Making Beam Search Work for Creative Tasks</h2> <p>One interesting challenge emerged: basic beam search seems great at finding the ‚Äúoptimal‚Äù solution but not so great at generating diverse, creative outputs‚Äîmy first attempts returned stories that were way too similar, sometimes differing by only a single word. Taking inspiration from the <a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute">blog post</a> mentioned earlier (see the DVTS section), I modified the algorithm to maintain independent ‚Äúbeams‚Äù of stories that develop separately, rather than constantly comparing and filtering them all together. This helped preserve more variety in the outputs.</p> <h2 id="the-computing-challenge">The Computing Challenge</h2> <p>Were you wondering where I spent the other 40% of my time? Well, here‚Äôs where it gets interesting: while generating a single story from a language model takes just a couple of seconds on a GPU (I tested on a T4), the full test-time computation process is much more intensive. With a modest-sized (and not particularly impressive anymore) model like GPT2, the entire process takes 3‚Äì5 minutes. Scale up to Llama 3.2 1B (my original target model) and you‚Äôll get significantly better stories, but you‚Äôll also be looking at 30+ minutes of processing time! That‚Äôs why I scaled it down for my demo.</p> <p>I‚Äôm sure my code can be optimized further, but even after some thorough refactoring to process data in batches wherever possible, I only improved runtime by about 20%. The iterative steps just take a while‚Äîat least compared to what we‚Äôre used to when using LLMs directly. I believe researchers are putting effort into embedding these search algorithms in the network architectures themselves, but I‚Äôm not an expert in this field, so don‚Äôt quote me on that!</p> <hr/> <h2 id="what-i-learned">What I Learned</h2> <p>Test-time computing offers a fascinating alternative to the ‚Äúbigger is better‚Äù mindset that often dominates LLM and AI development. While it does introduce significant computational overhead, it enables smaller models to produce higher-quality outputs through careful evaluation and refinement. Imagine how ‚Äúsmall model, big compute‚Äù could also help with things like dialogue systems, or specialized domain tasks; anything really that would benefit from ‚Äúthinking longer and in steps‚Äù.</p> <p>Of course, we have to acknowledge the elephant in the room: more inference steps cost more time, so if you need super-quick responses, you might be better off with bigger models. In practice, it will probably be a combination of both‚Äìas we keep pushing the limits of model scaling, these methods for making smarter use of inference-time compute will likely become increasingly important‚Äîespecially if new algorithms and hardware optimizations can streamline the process.</p> <p>For story generation specifically, in this experiment beam search seems to be very effective at finding ‚Äúoptimal‚Äù stories according to our metrics‚Äîthough maintaining creativity and diversity requires some clever tweaks to the standard approach. Perhaps other methods that would preference exploration over exploitation would work better. I do have a soft spot for Monte Carlo methods, so I‚Äôll try to find another <del>excuse</del> project to use a MC tree search next.</p> <h3 id="takeaways">Takeaways</h3> <ul> <li><strong>Small Models, Big Thinking</strong>: With enough ‚Äúthinking time,‚Äù a modest LLM can punch above its weight class.</li> <li><strong>Balancing Creativity &amp; Optimality</strong>: Beam search seems to be better at finding an optimal solution. Other methods might preference exploration more.</li> <li><strong>Real-World Viability</strong>: Extra processing time isn‚Äôt always ideal, but for certain applications it can be worth the trade-off.</li> </ul> <p>Feel free to tinker with the code, clone the project, or adapt it for your own experiments. I‚Äôd love to hear about your successes, hiccups, and any wild new ideas you come up with!</p>]]></content><author><name></name></author><category term="transformers"/><category term="llms"/><category term="test-time-compute"/><category term="beam-search"/><summary type="html"><![CDATA[Exploring test-time compute and beam search with a story generator app]]></summary></entry><entry><title type="html">If your company is not doing ML, it (probably) won‚Äôt start any time soon either</title><link href="https://ssalb.github.io/blog/2024/company-no-ml/" rel="alternate" type="text/html" title="If your company is not doing ML, it (probably) won‚Äôt start any time soon either"/><published>2024-03-22T09:00:00+00:00</published><updated>2024-03-22T09:00:00+00:00</updated><id>https://ssalb.github.io/blog/2024/company-no-ml</id><content type="html" xml:base="https://ssalb.github.io/blog/2024/company-no-ml/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-03-22-no-ml/cover-480.webp 480w,/assets/img/blog/2024-03-22-no-ml/cover-800.webp 800w,/assets/img/blog/2024-03-22-no-ml/cover-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2024-03-22-no-ml/cover.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Photo by Lucas Fonseca: https://www.pexels.com/photo/man-in-front-of-monitor-2239655/ </div> <p>A recurrent struggle among ML practitioners such as Data Scientists and ML Engineers is that, despite job descriptions and conversations during the interview process, after some time at their job they find themselves with the feeling of doing tons of different things except what they were most excited about - building some ML models.</p> <p>I‚Äôve been in that situation myself, and have heard it over and over again from friends, colleagues and students. The solution? <em>‚ÄúChange jobs!‚Äù</em> you might think, but the reality is that this is often not the best solution.</p> <p>In this article, I would like to share my point of view on why this is so common and use it to illustrate how you can make a better decision by having a wider perspective. And, if after all you still decide to go out there and find a new job (I‚Äôm not suggesting either option is right), this could help you identify good alternatives more easily.</p> <hr/> <h2 id="why-is-my-company-not-investing-more-in-ml">Why is my company not investing more in ML?</h2> <p>If you‚Äôre a professional with ML skills, you are most likely well aware of the potential machine learning has to revolutionize various industries. But have you noticed that not all companies are jumping on the bandwagon? In fact, many companies, perhaps yours, are not doing much machine learning despite having some Data Scientists in their lines.</p> <p>Some arguments I often hear are:</p> <ul> <li><em>‚ÄúWe don‚Äôt have the infrastructure for it.‚Äù</em></li> <li><em>‚ÄúThe quality of our data is insufficient.‚Äù</em></li> </ul> <p>But I would argue that it‚Äôs almost never related to the organization‚Äôs maturity or incompetence, because both of these problems are straightforward to fix.</p> <p>So why are some companies not adopting machine learning? The answer is simple: <strong>how companies make money is what drives their behavior.</strong></p> <p>Big Tech companies and machine learning startups do lots of machine learning because it impacts their revenue. That‚Äôs not always the case in other industries.</p> <p>If your company is not making money directly from machine learning, it‚Äôs unlikely they will invest in it significantly regardless of their original intentions. Even if they are aware of the potential benefits, the costs associated with implementing and maintaining machine learning systems may not justify the return on investment at first sight.</p> <hr/> <h2 id="what-can-you-do-about-it">What can you do about it?</h2> <p>If you‚Äôre an ML practitioner currently working at a company that‚Äôs not doing enough machine learning (or not at all), I would completely understand if you are feeling frustrated and asking yourself whether you should look for a new employer.</p> <p>Although in principle neither option is wrong (staying or leaving), I would strongly suggest that you don‚Äôt rush this decision. By analyzing companies under this lens just described above, it‚Äôs possible to conclude that if you blindly jump to another job, chances are you‚Äôll end up in the same situation.</p> <p>So what can you do about it? Here are some tips:</p> <h4 id="consider-the-opportunities">Consider the opportunities</h4> <p>If your company is not doing machine learning, that doesn‚Äôt necessarily mean there are no opportunities for you. It‚Äôs important to evaluate the potential for career development and decide whether the opportunities available are beneficial for your career goals long-term.</p> <p>Personally, working as a Data Scientist at companies whose business model was based on software is where I improved my software/data engineering skills the most, which have been incredibly useful. On the other hand, working at companies with other ‚Äúnon-tech‚Äù business models was when I gained tons of experience managing projects, budgets, stakeholders, and processes.</p> <h4 id="analyze-companies-under-this-lens">Analyze companies under this lens</h4> <p>Perhaps, after considering all opportunities currently available, you still decide the best you can do for your future is to look for a new challenge. In this case, understanding the business model of your potential employers can be incredibly useful for anticipating the kind of role you‚Äôll actually have, regardless of the job description. You can also use this point of view to formulate questions while interviewing for new jobs.</p> <hr/> <h2 id="final-thoughts">Final thoughts</h2> <p>In this article, I‚Äôve used this prism (<em>how does company X make money?</em>) to analyze why some companies are not where they said they want to be regarding ML development and implementation. But in fact, you can use the same lens to understand many other situations, such as:</p> <ul> <li>Which departments are likely to experience the biggest growth over time.</li> <li>Where you, as a Data professional, can have the largest impact with your work.</li> </ul> <p>By applying this mindset, you can make more informed career decisions and set yourself up for success in the long run.</p>]]></content><author><name></name></author><category term="career-development"/><category term="opinion"/><summary type="html"><![CDATA[Photo by Lucas Fonseca: https://www.pexels.com/photo/man-in-front-of-monitor-2239655/]]></summary></entry></feed>